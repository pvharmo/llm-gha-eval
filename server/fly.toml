# fly.toml app configuration file generated for server-vllm on 2024-09-19T12:08:20-04:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'server-vllm'
primary_region = 'ord'
# vm.size = "a100-40gb"

[build]
  image = "vllm/vllm-openai:v0.6.1.post2"

[experimental]
  entrypoint = "/start.sh"

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = 'stop'
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

[mounts]
  source = "models"
  destination = "/root/.cache/huggingface"
  initial_size = "100gb"

[[vm]]
  memory = '512mb'
  cpu_kind = 'shared'
  cpus = 1
  processes = ["app"]
  gpu_kind = 'a10'
  gpus = 1

[[files]]
  guest_path = "/start.sh"
  local_path = "start.sh"